Andrew Garber
Professor Tarnoff
ENGL1H

My goal for the remediation of "Philip"(Philosophy Bot), the AI that was created for the podcast project to philosophically debate issues to improve the arguments used, was to improve the prompting of the Perplexity API to improve the quality of the sources cited as well as to improve the AI's ability to understand the context of the conversation and respond more appropriately. Previously, the AI would cite links that didn't exist, as well as myopically focusing on a single example of the argument, and drill down on that forever, instead of more generally addressing the argument with reasoned counter-arguments. An example of this was the "There is no common good" argument I offered Philip, which within just a few messages back and forth became an argument about Rousseaue's specific conception of the general will that I was unable to shake Philip from.

One of the two primary changes I made was to switch form the old Perplexity Llama3.1-sonar-large-128k-online model, which was a perplexity specific implementation of Meta's Llama 3.1 model, to the new Perplexity Sonar-Pro model, an in-house model that makes it web-search centric instead of a more general purpose model with Perplexity's web search bolted on. I can't take significant credit for the performance improvements based on this change, but it was a significant improvement. This came with minor prompting changes to make it very explicit to output sources as a main requirement of each message response, which ensured that no message did not cite sources. Unfortunately, the model does sometimes invent sources(as a function of its training data mixing with the web search functionality, it sometimes cites sources that don't exist), but the percentage of good sources cited significantly improved from the last iteration.

The other big change I made was to how the previous conversation history was handled. Previously, I simply load it in as part of the prompt without much instruction on how to handle it - this worked and gave it context on what had already been discussed, but led to it consistently diving deep into rabbit holes without using them within the overall point the user is trying to discuss. I made some prompt changes that took the first message, which states what the user wants to discuss, and then tells the AI to use the conversation history to inform its response in relation to that original intent. This was a significant improvement, as it allowed the AI to more easily take a broader view of the argument and not get stuck in a single example/rabbit hole - unfortunately, it sometimes refused to go down the rabbit hole and argue the fine details, but this is something that could be fixed in a future iteration with perhaps some mode toggle.

Overall, I think that the remediation of Philip to Philip.2.0 was a success, and that he has become more helpful and more reliable as a source of information and as a debate partner. Philip could be improved in the future by having tuned prompts specifically for different parts of the argument, but for most possible use cases this is probably excessive.
